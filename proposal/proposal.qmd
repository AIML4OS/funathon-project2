---
title: "proposal"
---

## Data

Two possibilities:

- Use our open data from the registry.
- Generate synthetic data in English using `llm.lab` ("generate me plausible codes for code XX ") and sample using real label distribution from our data.

Nice task for Mat√©o Morin (intern) who arrives in February and will be working on synthetic data.
The generation process would not be part of the subject but will be disclosed for interested readers.

## Goal

- Task 1: Understand how to train and optimize a `torchTextClassifiers` classifier with overview of different features: custom tokenizer, explainability (**a supervised learning approach**):
  - Step 1:  from data preprocessing/splitting
  - Step 2: show how to choose right hyperparameters via grid search - we release the weights to avoid retraining
  - Step 3: log and promote a wrapper on MLFlow
  - Step 4: deploy a model using an API
  - no GPU needed, unless for advanced users that want to train themselves the model (request needed) 

- Task 2: Understand a simple RAG system by building it, step-by-step (**a few-shot learning approach**)
  - Step 1: embed a text (how and why)
  - Step 2: analyse the retrieval
  - Step 3: prompt engineering
  - some batched calls to llm.lab will be involved

One notebook for each - independent but that should be done "sequentially". Utils functions will be imported from _*.py_ files.

# Tools

- TTC package for supervised text classification (based on PyTorch and Lightning - deep learning frameworks)
- llm.lab API for RAG, as well as some `qdrant` for vectorial database
- Python
