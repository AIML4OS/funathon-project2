---
title: "proposal"
---

## Data

- One choice is to use our data open data from the registry.
- Otherwise, we will generate synthetic data in English using `llm.lab` ("generate me plausible codes for code XX ") and sample using real label distribution from our data.

Nice task for Mat√©o Morin (intern) that arrives in February and will be working on synthetic data.
The generation process will not be part of the subject but will be disclosed for interested readers.

## Goal

- Understand how to train and optimize a `torchTextClassifiers` classifier with overview of different features: custom tokenizer, explainability (**a supervised learning approach**):
  - start from data processing/splitting
  - show how to choose right hyperparameters via grid search - we release the weights to avoid retraining
  - log and promote a wrapper on MLFlow
  - no GPU needed, unless for advanced users that want to train themselves the model (request needed) 

- Understand a simple RAG system by building it, step-by-step (**a few-short learning approach**)
  - embed a text
  - analyse the retrieval
  - prompt engineering
  - some batched calls to llm.lab will be involved

One notebook for each - independent but that should be done "sequentially". Utils functions will be imported from _*.py_ files.

# Tools

- TTC package for supervised text classification (based on PyTorch and Lightning - deep learning frameworks)
- llm.lab API for RAG, as well as some `qdrant` for vectorial database
- Python
